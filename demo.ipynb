{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df00fa1c-c24e-4caf-9ee4-3b400e68a057",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40f62e1-1738-4262-b383-f85589022f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moon/miniconda3/envs/donut/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import transformers\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hidde logs\n",
    "transformers.logging.disable_default_handler()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load our model from Hugging Face\n",
    "processor = DonutProcessor.from_pretrained(\"Mo-oN/donut-base-DO\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"/data/moon/donut-base-DO/checkpoint-4800/\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e723f3-3b11-41b7-967e-695ed92ad9ff",
   "metadata": {},
   "source": [
    "## Load delivery order test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35cfa642-0b4d-4f2e-8ce1-6bb04fd798b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from dataset import DeliveryOrder\n",
    "filepath = \"./\"\n",
    "dataset = DeliveryOrder(filepath, split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef8777-782d-4685-bb92-518ce32eaa9d",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83ce28fd-ea11-40f5-b3a6-6a92e6f9c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = dataset.preprocess_documents_for_donut()\n",
    "print(len(processed_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afff3f-1e32-4542-9a56-b8cb88dabcbb",
   "metadata": {},
   "source": [
    "## Inference on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3acc0ddb-0ab5-43e6-9b4b-24543ae35012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(sample, model=model, processor=processor):\n",
    "    # prepare inputs\n",
    "    pixel_values = torch.tensor(test_sample[\"pixel_values\"]).unsqueeze(0)\n",
    "    task_prompt = \"<s>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # run inference\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    # process output\n",
    "    prediction = processor.batch_decode(outputs.sequences)[0]\n",
    "    prediction = processor.token2json(prediction)\n",
    "\n",
    "    # load reference target\n",
    "    target = processor.token2json(test_sample[\"target_sequence\"])\n",
    "    return prediction, target, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6ac5066d-70ea-4a52-b4cd-68d4897a6ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_546845/2043591347.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values = torch.tensor(test_sample[\"pixel_values\"]).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:\n",
      " {'SHIPPER': 'MK TRADING LLC', 'CARRIER': 'HYUNDAI MERCHANT MARINE (AMERICA) INC', 'VESSEL': 'HYUNDAI MARS 037W', 'POL': 'LONG BEACH,CA', 'POD': 'BUSAN, KOREA', 'ETD': '02/01/2023', 'ETA': '02/17/2023', 'COMMODITY': ''}\n",
      "Prediction:\n",
      " {'SHIPPER': 'MK TRADING LLC', 'CARRIER': 'HYUNDAI MERCHANT MARINE (AMERICA) INC', 'VESSEL': 'HYUNDAI MARS 037W', 'POL': 'LONG BEACH,CA', 'POD': 'BUSAN, KOREA', 'ETD': '02/01/2023', 'ETA': '02/17/2023', 'COMMODITY': ''}\n"
     ]
    }
   ],
   "source": [
    "test_sample = processed_dataset[1]\n",
    "prediction, target, outputs = run_prediction(test_sample)\n",
    "print(f\"Reference:\\n {target}\")\n",
    "print(f\"Prediction:\\n {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5e00f-d1d5-4141-bfa3-fa8939d4bcfe",
   "metadata": {},
   "source": [
    "## Visualize cross-attention results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d24ff48b-3124-4461-8fe2-42cac3f0790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_txt = {}\n",
    "for i, t in enumerate(outputs.sequences[0]):\n",
    "    dec_txt[i] = processor.decode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "add8437d-cfca-4e91-80ae-c15649005790",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [2,3,4,5,6] # SHIPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a34e5546-aed6-415f-987f-019c4e12de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vis_utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "def vis(img_path, tkn_indices, decoder_cross_attentions, final_w=1920, final_h=2560):\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (final_w, final_h))\n",
    "    \n",
    "    max_area_box, thres_heatmap, agg_heatmap = attn_heatmap(tkn_indices, decoder_cross_attentions)\n",
    "    raw_heatmap = deepcopy(agg_heatmap)\n",
    "    raw_image = deepcopy(image)\n",
    "\n",
    "    # x1, y1, x2, y2 = box\n",
    "    x1, y1, x2, y2 = max_area_box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (255,100,200), 2)\n",
    "\n",
    "    heatmap_img = cv2.applyColorMap(thres_heatmap, cv2.COLORMAP_JET)\n",
    "    super_imposed_img = cv2.addWeighted(heatmap_img, 0.2, image, 0.8, 0)\n",
    "    # super_imposed_img = cv2.addWeighted(heatmap_img, 0.5, image, 0.5, 0)\n",
    "\n",
    "    super_imposed_raw_heatmap_img = cv2.addWeighted(cv2.applyColorMap(raw_heatmap, cv2.COLORMAP_JET), 0.2, raw_image, 0.8, 0)\n",
    "    # super_imposed_raw_heatmap_img = cv2.addWeighted(cv2.applyColorMap(raw_heatmap, cv2.COLORMAP_JET), 0.5, raw_image, 0.5, 0)\n",
    "\n",
    "    # display image with heatmap\n",
    "    images_2_disp = [super_imposed_raw_heatmap_img, super_imposed_img]\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    # plt.figure(figsize=(30, 30))\n",
    "    columns = 2\n",
    "    for i, img2disp in enumerate(images_2_disp):\n",
    "        plt.subplot(len(images_2_disp) // columns + 1, columns, i + 1)\n",
    "        plt.imshow(img2disp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c334f-15b8-4346-b277-c64296951c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./test/PE_112.png\"\n",
    "vis(img_path, indices, outputs.cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5e13-0911-44a6-857c-67b9100afe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "donut",
   "language": "python",
   "name": "donut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
